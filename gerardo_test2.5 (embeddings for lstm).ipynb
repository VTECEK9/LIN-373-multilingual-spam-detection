{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bbf88c86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\calig\\anaconda3\\lib\\site-packages\\scipy\\__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Dropout, LSTM, Dense\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Load your dataset\n",
    "data_df = pd.read_csv('data-en-hi-de-fr.csv')\n",
    "data_df.dropna(inplace=True)\n",
    "data_df.drop_duplicates(inplace=True)\n",
    "data_df.rename(columns={\"Category\": \"labels\", \"Message\": \"text\"}, inplace=True)\n",
    "\n",
    "# Label encoding\n",
    "le = LabelEncoder()\n",
    "data_df['labels'] = le.fit_transform(data_df.labels)\n",
    "\n",
    "# Reset indices after preprocessing to ensure alignment\n",
    "data_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cced1886",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load spacy models for different languages\n",
    "nlp_en = spacy.load('en_core_web_sm')\n",
    "nlp_fr = spacy.load('fr_core_news_sm')\n",
    "nlp_de = spacy.load('de_core_news_sm')\n",
    "\n",
    "def preprocess_text(text, nlp):\n",
    "    doc = nlp(text.lower().strip())\n",
    "    return \" \".join([token.lemma_ for token in doc if not token.is_punct and not token.is_stop and not token.like_num])\n",
    "\n",
    "# Apply preprocessing\n",
    "data_df['processed_text_en'] = data_df['text'].apply(preprocess_text, nlp=nlp_en)\n",
    "data_df['processed_text_fr'] = data_df['text_fr'].apply(preprocess_text, nlp=nlp_fr)\n",
    "data_df['processed_text_de'] = data_df['text_de'].apply(preprocess_text, nlp=nlp_de)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d1532a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_en = Tokenizer(num_words=5000)\n",
    "tokenizer_fr = Tokenizer(num_words=5000)\n",
    "tokenizer_de = Tokenizer(num_words=5000)\n",
    "\n",
    "tokenizer_en.fit_on_texts(data_df['processed_text_en'])\n",
    "tokenizer_fr.fit_on_texts(data_df['processed_text_fr'])\n",
    "tokenizer_de.fit_on_texts(data_df['processed_text_de'])\n",
    "\n",
    "sequences_en = tokenizer_en.texts_to_sequences(data_df['processed_text_en'])\n",
    "sequences_fr = tokenizer_fr.texts_to_sequences(data_df['processed_text_fr'])\n",
    "sequences_de = tokenizer_de.texts_to_sequences(data_df['processed_text_de'])\n",
    "\n",
    "max_sequence_len = 150\n",
    "X_seq_en = pad_sequences(sequences_en, maxlen=max_sequence_len)\n",
    "X_seq_fr = pad_sequences(sequences_fr, maxlen=max_sequence_len)\n",
    "X_seq_de = pad_sequences(sequences_de, maxlen=max_sequence_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0023257",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings(path):\n",
    "    if 'glove' in path:\n",
    "        return KeyedVectors.load_word2vec_format(path, binary=False, unicode_errors='ignore', no_header=True)\n",
    "    return KeyedVectors.load_word2vec_format(path, binary=False, unicode_errors='ignore')\n",
    "\n",
    "embeddings_en = load_embeddings('glove.6B.100d.txt')  # Assuming GloVe for English\n",
    "embeddings_fr = load_embeddings('cc.fr.300.vec')      # Assuming fastText for French\n",
    "embeddings_de = load_embeddings('cc.de.300.vec')      # Assuming fastText for German"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8380dde2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding_matrix(embeddings, tokenizer, embedding_dim):\n",
    "    embedding_matrix = np.zeros((len(tokenizer.word_index) + 1, embedding_dim))\n",
    "    for word, i in tokenizer.word_index.items():\n",
    "        try:\n",
    "            embedding_vector = embeddings.get_vector(word)\n",
    "            if embedding_vector is not None:\n",
    "                embedding_matrix[i] = embedding_vector\n",
    "        except KeyError:\n",
    "            continue\n",
    "    return embedding_matrix\n",
    "\n",
    "embedding_matrix_en = get_embedding_matrix(embeddings_en, tokenizer_en, 100)  # GloVe dimensions\n",
    "embedding_matrix_fr = get_embedding_matrix(embeddings_fr, tokenizer_fr, 300)  # fastText dimensions\n",
    "embedding_matrix_de = get_embedding_matrix(embeddings_de, tokenizer_de, 300)  # fastText dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2a81265",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 133ms/step - accuracy: 0.8785 - loss: 0.4448 - val_accuracy: 0.8476 - val_loss: 0.3189\n",
      "Epoch 2/5\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 115ms/step - accuracy: 0.8825 - loss: 0.2841 - val_accuracy: 0.8906 - val_loss: 0.2861\n",
      "Epoch 3/5\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 167ms/step - accuracy: 0.8917 - loss: 0.2548 - val_accuracy: 0.8906 - val_loss: 0.2780\n",
      "Epoch 4/5\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 164ms/step - accuracy: 0.8981 - loss: 0.2364 - val_accuracy: 0.8920 - val_loss: 0.2682\n",
      "Epoch 5/5\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 150ms/step - accuracy: 0.9148 - loss: 0.2252 - val_accuracy: 0.8809 - val_loss: 0.2626\n",
      "Epoch 1/5\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 167ms/step - accuracy: 0.8810 - loss: 0.4030 - val_accuracy: 0.8837 - val_loss: 0.2658\n",
      "Epoch 2/5\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 157ms/step - accuracy: 0.8908 - loss: 0.2296 - val_accuracy: 0.8920 - val_loss: 0.2563\n",
      "Epoch 3/5\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 159ms/step - accuracy: 0.9194 - loss: 0.2153 - val_accuracy: 0.9169 - val_loss: 0.2377\n",
      "Epoch 4/5\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 182ms/step - accuracy: 0.9233 - loss: 0.2051 - val_accuracy: 0.9321 - val_loss: 0.2206\n",
      "Epoch 5/5\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 180ms/step - accuracy: 0.9386 - loss: 0.1776 - val_accuracy: 0.9266 - val_loss: 0.2011\n",
      "Epoch 1/5\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 158ms/step - accuracy: 0.8685 - loss: 0.4028 - val_accuracy: 0.8878 - val_loss: 0.2694\n",
      "Epoch 2/5\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 148ms/step - accuracy: 0.9125 - loss: 0.2182 - val_accuracy: 0.9058 - val_loss: 0.2550\n",
      "Epoch 3/5\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 166ms/step - accuracy: 0.9182 - loss: 0.2170 - val_accuracy: 0.9266 - val_loss: 0.2389\n",
      "Epoch 4/5\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 187ms/step - accuracy: 0.9334 - loss: 0.1943 - val_accuracy: 0.9404 - val_loss: 0.1973\n",
      "Epoch 5/5\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 175ms/step - accuracy: 0.9439 - loss: 0.1692 - val_accuracy: 0.9432 - val_loss: 0.1860\n",
      "Evaluation for English Model:\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 40ms/step - accuracy: 0.8932 - loss: 0.2295\n",
      "Evaluation for French Model:\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 46ms/step - accuracy: 0.9240 - loss: 0.1869\n",
      "Evaluation for German Model:\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 41ms/step - accuracy: 0.9378 - loss: 0.1749\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.16990873217582703, 0.9392764568328857]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_model(embedding_matrix, max_length, lstm_units):\n",
    "    model = Sequential([\n",
    "        Embedding(input_dim=embedding_matrix.shape[0], output_dim=embedding_matrix.shape[1], input_length=max_length, trainable=False),\n",
    "        Dropout(0.2),\n",
    "        LSTM(lstm_units, dropout=0.2, recurrent_dropout=0.2),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Split data\n",
    "train_x, test_x, train_y, test_y = train_test_split(X_seq_en, data_df.labels, test_size=0.3, random_state=123)\n",
    "train_x_fr, test_x_fr, train_y_fr, test_y_fr = train_test_split(X_seq_fr, data_df.labels, test_size=0.3, random_state=123)\n",
    "train_x_de, test_x_de, train_y_de, test_y_de = train_test_split(X_seq_de, data_df.labels, test_size=0.3, random_state=123)\n",
    "\n",
    "# Build models\n",
    "model_en = build_model(embedding_matrix_en, max_sequence_len, 64)\n",
    "model_fr = build_model(embedding_matrix_fr, max_sequence_len, 64)\n",
    "model_de = build_model(embedding_matrix_de, max_sequence_len, 64)\n",
    "\n",
    "# Train and evaluate\n",
    "model_en.fit(train_x, train_y, epochs=5, validation_split=0.2)\n",
    "model_fr.fit(train_x_fr, train_y_fr, epochs=5, validation_split=0.2)\n",
    "model_de.fit(train_x_de, train_y_de, epochs=5, validation_split=0.2)\n",
    "\n",
    "# Example for evaluation - can add detailed evaluation as necessary\n",
    "print(\"Evaluation for English Model:\")\n",
    "model_en.evaluate(test_x, test_y)\n",
    "print(\"Evaluation for French Model:\")\n",
    "model_fr.evaluate(test_x_fr, test_y_fr)\n",
    "print(\"Evaluation for German Model:\")\n",
    "model_de.evaluate(test_x_de, test_y_de)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
